# Strategy Discovery Engine — Build Plan

> **Goal**: Build an automated system that generates, backtests, evaluates, and iterates on trading strategies — evolving from simple parameter sweeps to LLM-powered strategy creation.

---

## Data Strategy: Clean Break from Legacy Results

### Problem: Existing Database Is Contaminated

The current `test_runs` table contains results from multiple backtest configurations:
- **Broken delay=1 runs** — phantom profits (e.g. StochRSI QQQ showing 44.9% vs corrected 0.99%)
- **Zero-spread runs** — unrealistic cost assumptions
- **Mixed settings** — `spread` and `execution_delay` columns exist but older runs have 0/NULL

The `research_insights.md` file (auto-generated by `analyze_results.py`) prominently surfaces these inflated results as "best strategies." It's actively misleading.

### Solution: Separate Experiments Table

The sweep engine writes to a **new `experiments` table**, not `test_runs`. This gives:
- **Clean data**: Every row guaranteed to use `spread=0.0003, delay=0`
- **No pollution**: Old manual exploration data stays in `test_runs` untouched
- **Purpose-built schema**: Adds fields the old table lacks (Sharpe, composite score, experiment_id, validation status)

```sql
CREATE TABLE IF NOT EXISTS experiments (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    experiment_id TEXT,          -- Groups related runs: "sweep_001", "llm_iter_003"
    strategy TEXT,               -- Strategy class name
    strategy_source TEXT,        -- "existing", "composable", "llm_generated"
    symbol TEXT,
    timeframe TEXT,
    parameters TEXT,             -- JSON
    return_pct REAL,
    annualised_return REAL,
    max_drawdown REAL,
    total_trades INTEGER,
    trades_per_year REAL,
    win_rate REAL,
    profit_factor REAL,
    sharpe REAL,
    score REAL,                  -- Composite score from scoring function
    train_period TEXT,           -- "2020-01-01:2023-12-31"
    test_period TEXT,            -- "2024-01-01:2025-12-31"
    test_return_pct REAL,        -- Out-of-sample return (Phase 2)
    validation_status TEXT,      -- "pending", "passed", "marginal", "rejected"
    validation_details TEXT,     -- JSON with walk-forward, multi-asset results
    parent_experiment_id TEXT,   -- What this was iterated from (for LLM agent lineage)
    created_at TEXT,
    spread REAL DEFAULT 0.0003,
    execution_delay INTEGER DEFAULT 0
);
```

### What Happens to research_insights.md

**It's retired.** The automated system doesn't need a human-readable markdown report generated from contaminated data. Instead:

- **The agent queries the `experiments` table directly** to understand what's been tried
- **A summary function** builds concise context for the LLM agent prompt (Phase 4)
- **You see results** via a simple "top N candidates" report generated on demand, not a 400-line markdown file

The existing `analyze_results.py` and `research_insights.md` remain in the codebase but are not part of the new pipeline. They served their purpose during manual exploration.

### What the Agent Needs to Learn From Past Results

Instead of a markdown file, the system provides query functions:

```python
class ExperimentTracker:
    def get_top_candidates(self, n=10):
        """Top N experiments by score, validated only."""

    def get_failures_for_strategy(self, strategy_name):
        """All failed experiments for a strategy — what didn't work and why."""

    def get_experiments_by_asset(self, symbol):
        """Everything tried on a symbol — helps avoid re-testing dead ends."""

    def get_summary_for_llm(self, max_results=20):
        """Builds a concise text summary for the LLM agent prompt.
        Includes: top performers, worst failures, untested combinations."""

    def has_been_tested(self, strategy, symbol, timeframe, params_hash):
        """Avoid duplicate work — skip combinations already tested."""
```

The `get_summary_for_llm()` method replaces research_insights.md. It's generated fresh from clean data every time the agent runs, not a stale file on disk.

---

## Why Build This

We've proven that the infrastructure works (100+ live trades, 5+ days uptime) and that the backtester is trustworthy (`--spread 0.0003 --delay 0` matches live execution). But manual strategy development has hit diminishing returns — every indicator combination we've tested on liquid US ETFs produces ~zero alpha after costs.

The search space is too large for manual exploration:
- 9 indicators with tuneable parameters
- Multiple assets, timeframes, and entry/exit patterns
- Multi-indicator confluence (untested entirely)
- Zone-based vs breakout vs hybrid logic

An automated system can explore this space systematically, test thousands of combinations with proper overfitting controls, and eventually use LLM reasoning to generate genuinely novel approaches.

---

## What We Already Have

### Backtester (Python API — no CLI needed)

```python
from backend.engine.backtester import Backtester

backtester = Backtester(
    data=df,                    # pd.DataFrame with OHLCV columns
    strategy_class=MyStrategy,  # Class reference, not instance
    parameters=params_dict,     # Strategy-specific config
    initial_capital=10000.0,
    spread=0.0003,              # 0.03% = matches live execution
    execution_delay=0           # MUST be 0 (delay=1 is broken)
)
results = backtester.run()
```

**Results dict** returned by `run()`:
```python
{
    "return_pct": float,       # Total return %
    "max_drawdown": float,     # Max peak-to-trough %
    "total_trades": int,       # Round-trip count
    "win_rate": float,         # 0.0 - 1.0
    "avg_win": float,          # Average winning trade $
    "avg_loss": float,         # Average losing trade $
    "profit_factor": float,    # Sum wins / abs(sum losses)
    "equity_curve": list,      # [{time, equity}, ...]
    "trade_history": list,     # [{symbol, side, qty, entry, exit, pnl, timestamp}, ...]
    "orders": list,            # All filled orders
}
```

Key: `Backtester` takes a `data` DataFrame, not a symbol string. Data can be fetched once and reused across hundreds of parameter combinations.

### Indicator Library (9 indicators, two interface types)

**Vectorized functions** (take `pd.Series`, return `pd.Series/DataFrame` — easy to compose):

| Function | File | Signature | Returns |
|---|---|---|---|
| `adx()` | `backend/indicators/adx.py` | `(high, low, close, period=14)` | `pd.Series` |
| `atr()` | `backend/indicators/atr.py` | `(high, low, close, period)` | `pd.Series` |
| `sma()` | `backend/indicators/sma.py` | `(series, period)` | `pd.Series` |
| `macd()` | `backend/indicators/macd.py` | `(series, fast=12, slow=26, signal=9)` | `DataFrame{macd, signal, histogram}` |
| `bollinger_bands()` | `backend/indicators/bollinger.py` | `(series, period=20, std_dev=2.0)` | `DataFrame{upper, middle, lower}` |
| `donchian_channels()` | `backend/indicators/donchian.py` | `(high, low, entry_period, exit_period)` | `DataFrame{upper_entry, lower_entry, upper_exit, lower_exit}` |
| `chop_index()` | `backend/indicators/chop.py` | `(high, low, close, period=14)` | `pd.Series` (>61.8 = chop, <38.2 = trend) |
| `rsi()` | `backend/indicators/rsi.py` | `(series, period=14)` | `pd.Series` |

**Stateful classes** (`.update(price)` per bar — requires iterative loop):

| Class | File | State | Outputs |
|---|---|---|---|
| `StochRSI` | `backend/indicators/stoch_rsi.py` | RSI history, smoothing buffers | `.k`, `.d` (0-100), `.ready` |
| `RSI` | `backend/indicators/rsi.py` | Gain/loss averages | `.value` (0-100), `.ready` |
| `BollingerBands` | `backend/indicators/bollinger.py` | Price window | `.upper`, `.middle`, `.lower`, `.ready` |

**Not yet implemented**: OBV, VWAP, volume-based indicators.

### Strategy Base Class

```python
# backend/engine/strategy.py
class Strategy(ABC):
    def __init__(self, data, events, parameters, initial_cash=10000.0, broker=None):
        self.data = data
        self.parameters = parameters
        self.broker = broker  # PaperTrader for backtest, LiveBroker for prod
        self.position = 0
        # ...

    @abstractmethod
    def on_data(self, index, row):  # Called every bar by Backtester
        pass

    @abstractmethod
    def on_event(self, event):  # Called on economic events
        pass

    def buy(self, price, size, timestamp, stop_loss, take_profit):
        return self.broker.place_order(...)

    def sell(self, price, size, timestamp, stop_loss, take_profit):
        return self.broker.place_order(...)
```

All existing strategies follow the pattern:
- `__init__` → extract params, calculate indicators
- `on_data(index, row)` → delegates to `on_bar(row, bar_index, full_data)`
- `on_bar(row, i, df)` → core logic: check stops, check entries, check exits

### Existing Strategies (22 files, 17 registered)

Registered in `STRATEGY_MAP` (`backend/runner.py:28-46`):
DonchianBreakout, BollingerBreakout, MACDBollinger, StochRSIMeanReversion,
StochRSINextOpen, StochRSILimit, HybridRegime, DonchianTrend, SimpleSMA,
StochRSIQuant, GammaScalping, RapidFireTest, AGoldenCross, RegimeGatedStoch,
SwingBreakout + dynamically: StochRSISniper, HybridRegimeV2.

Three strategy architecture patterns exist:
1. **Zone-based state machines** — StochRSIMeanReversion tracks `in_oversold_zone`/`in_overbought_zone`, enters on reversal crossing. Complex state, hard to decompose.
2. **Stateless condition checks** — DonchianBreakout checks `Close > entry_high` each bar, queries broker for position. Simple, easy to compose.
3. **Wrapper/decorator** — RegimeGatedStoch inherits from StochRSIMeanReversion, gates it with a regime filter. Already a form of composition.

### Database & Results Tracking

SQLite (`backend/research.db`) with tables:
- `test_runs` — strategy, symbol, timeframe, return_pct, max_drawdown, win_rate, total_trades, parameters (JSON), iteration_index, spread, execution_delay
- `equity_curves` — test_id → JSON equity time series
- `live_trade_log` — session_id, fill_price, signal_price, slippage, pnl
- `insights` — findings storage

Iteration tracking already exists: `test_id = "{strategy}_{symbol}_{timeframe}_{year}_v{iteration}"`.

### Batch Runner (existing)

`run_matrix()` in `runner.py` uses `multiprocessing.Pool` for parallel backtesting with yearly result splitting. However, strategy params are hardcoded in duplicated if/elif blocks. The sweep engine bypasses this by calling `Backtester` directly.

### Data Loading

`AlpacaDataLoader` (`backend/engine/alpaca_loader.py`) fetches OHLCV from Alpaca API.

Critical detail — **resampling is required** for some timeframes:
- 5m and 15m → must fetch 1m and resample
- 4h → must fetch 1h and resample
- 1m, 1h, 1d → fetch directly

This resampling logic is currently duplicated in `run_backtest()` and `worker_task()`. The sweep engine needs a clean extraction.

### Broker/Execution

`PaperTrader` (`backend/engine/paper_trader.py`):
- Spread model: buy at `price * (1 + spread/2)`, sell at `price * (1 - spread/2)`
- 1x leverage cap: position value cannot exceed equity (silent cap)
- Tracks `trade_history` with realized PnL per round-trip

---

## Architecture Overview

```
Phase 1: Sweep Engine
    Fetch data once per symbol/timeframe
    → Run Backtester N times with different params
    → Score & rank results
    → Store in experiments DB

Phase 2: Validation Framework
    Take Phase 1 winners
    → Walk-forward test (rolling windows)
    → Out-of-sample test (train 2020-2023 / test 2024-2025)
    → Multi-asset consistency check
    → Reject overfitting

Phase 3: Composable Strategy Framework
    Define entry/exit/filter building blocks
    → Auto-generate strategy combinations
    → Feed through Phase 1 + Phase 2 pipeline
    → Surface genuinely novel combinations

Phase 4: LLM Agent
    Claude analyses Phase 1-3 results
    → Hypothesises why strategies fail/succeed
    → Generates new strategy code
    → Runs through full pipeline
    → Iterates based on results
```

Each phase builds on the previous. Nothing is thrown away.

---

## Phase 1: Sweep Engine

### What It Does

Takes existing strategies, sweeps parameter combinations across multiple assets and timeframes, scores results, and ranks them. This is the workhorse that every subsequent phase builds on.

### Key Design Decisions

**Fetch data once, reuse many times.** A sweep of 200 parameter combinations for StochRSI on SPY 5m should make ONE Alpaca API call, not 200.

**Call Backtester directly.** Bypass `run_backtest()` and its hardcoded param defaults. Import `Backtester` and pass complete param dicts. This gives full control and avoids the duplicated if/elif blocks.

**Score by Sharpe ratio, not raw returns.** Raw returns reward volatility. Sharpe penalises inconsistency. A strategy returning 5% with 2% drawdown is better than one returning 10% with 15% drawdown.

### Components to Build

#### 1. Data Loader Utility

Extract the resampling logic from `runner.py` into a clean reusable function:

```python
# backend/engine/data_utils.py

def load_backtest_data(symbol: str, timeframe: str, start: str, end: str) -> pd.DataFrame:
    """
    Fetch and resample OHLCV data. Handles:
    - 5m/15m: fetch 1m, resample
    - 4h: fetch 1h, resample
    - 1m/1h/1d: fetch directly
    Returns a ready-to-use DataFrame for Backtester.
    """
```

This eliminates the duplicated resampling code in `run_backtest()` and `worker_task()`.

#### 2. Sweep Runner

```python
# backend/optimizer/sweep.py

class SweepEngine:
    def __init__(self, spread=0.0003, initial_capital=10000.0):
        self.spread = spread
        self.initial_capital = initial_capital
        self.results = []

    def run_sweep(self, strategy_class, param_grid, symbol, timeframe, start, end):
        """
        Run backtests for all parameter combinations.
        Fetches data once, runs Backtester N times.
        """
        # 1. Load data once
        data = load_backtest_data(symbol, timeframe, start, end)

        # 2. Generate parameter combinations
        combos = self._expand_grid(param_grid)

        # 3. Run each combination
        for params in combos:
            params['symbol'] = symbol
            bt = Backtester(data, strategy_class, params,
                           self.initial_capital, self.spread,
                           execution_delay=0)
            result = bt.run()
            result['params'] = params
            result['score'] = self._score(result)
            self.results.append(result)

        # 4. Sort by score
        self.results.sort(key=lambda r: r['score'], reverse=True)

    def _score(self, result):
        """Composite score. Returns -999 for disqualified results."""
        if result['total_trades'] < 30:
            return -999  # Not enough trades to be meaningful

        # Annualised Sharpe approximation from equity curve
        sharpe = self._calc_sharpe(result['equity_curve'])

        # Penalise complexity (more params = more overfitting risk)
        # param_count = len([v for v in result['params'].values()
        #                    if isinstance(v, (int, float))])
        # penalty = param_count * 0.05

        return sharpe

    def _expand_grid(self, param_grid):
        """Cartesian product of all parameter values."""
        from itertools import product
        keys = list(param_grid.keys())
        values = list(param_grid.values())
        return [dict(zip(keys, combo)) for combo in product(*values)]
```

#### 3. Experiment Tracker

Uses the `experiments` table defined in the Data Strategy section above. The tracker class wraps this table with query methods the rest of the system depends on:

```python
# backend/optimizer/experiment_tracker.py

class ExperimentTracker:
    """Central store for all automated backtest results.
    Replaces research_insights.md and the analyze_results.py pipeline."""

    def __init__(self, db_file="backend/research.db"):
        self.db_file = db_file
        self._ensure_table()

    def save(self, experiment_id, strategy, symbol, timeframe,
             params, results, strategy_source="existing"):
        """Save a single backtest result to the experiments table."""

    def get_top_candidates(self, n=10, min_trades=30,
                           validation_status="passed"):
        """Top N experiments by score. Only validated results by default."""

    def get_failures_for_strategy(self, strategy_name):
        """What didn't work for this strategy — params, assets, timeframes.
        Used by LLM agent to avoid repeating dead ends."""

    def get_untested_combinations(self, strategy, symbols, timeframes):
        """Identify gaps in the search space — what hasn't been tried yet."""

    def get_summary_for_llm(self, max_results=20):
        """Build a concise text summary for the LLM agent prompt.
        Replaces research_insights.md with clean, up-to-date data.

        Returns something like:
        'TOP 5 STRATEGIES:
         1. DonchianBreakout on XLE 4h: +3.2% ann, Sharpe 0.8, PASSED validation
         ...
         NOTABLE FAILURES:
         1. StochRSI on SPY/QQQ/IWM (all timeframes): ~0% after costs
         ...
         UNTESTED: OBV indicators, CHOP filter, multi-TF confluence'
        """

    def has_been_tested(self, strategy, symbol, timeframe, params_hash):
        """Skip combinations already tested. Prevents duplicate work
        across sweep runs and LLM iterations."""
```

This is the **single source of truth** for the entire system. Phase 1 writes to it, Phase 2 updates `validation_status`, Phase 3 writes composable results, Phase 4 reads summaries and writes LLM-generated results. All through the same table with `strategy_source` distinguishing origin.

#### 4. Sweep Configuration

```python
# Example: sweep StochRSI across params and assets
SWEEPS = [
    {
        "strategy": StochRSIMeanReversionStrategy,
        "symbols": ["SPY", "QQQ", "IWM", "XLE", "XBI", "EEM", "GLD", "TLT"],
        "timeframes": ["5m", "15m", "1h"],
        "param_grid": {
            "rsi_period": [7, 14, 21],
            "stoch_period": [7, 14, 21],
            "overbought": [70, 75, 80, 85],
            "oversold": [15, 20, 25, 30],
            "sl_atr": [1.5, 2.0, 2.5, 3.0],
            "skip_adx_filter": [True, False],
            "adx_threshold": [20, 25, 30],
        }
    },
    {
        "strategy": DonchianBreakoutStrategy,
        "symbols": ["SPY", "QQQ", "IWM", "XLE", "XBI", "EEM"],
        "timeframes": ["1h", "4h", "1d"],
        "param_grid": {
            "entry_period": [10, 20, 30, 55],
            "exit_period": [5, 10, 20],
            "stop_loss_atr": [1.5, 2.0, 3.0],
            "atr_period": [14, 20],
        }
    },
    {
        "strategy": MACDBollingerStrategy,
        "symbols": ["SPY", "QQQ", "IWM", "XLE"],
        "timeframes": ["15m", "1h", "4h"],
        "param_grid": {
            "macd_fast": [8, 12, 16],
            "macd_slow": [21, 26, 30],
            "macd_signal": [7, 9, 12],
            "bb_period": [15, 20, 25],
            "bb_std": [1.5, 2.0, 2.5],
            "sl_atr": [1.5, 2.0, 3.0],
        }
    }
]
```

**Scale check**: StochRSI grid = 3 * 3 * 4 * 4 * 4 * 2 * 3 = 3,456 combos per symbol/timeframe. Across 8 symbols and 3 timeframes = 82,944 backtests. At ~0.5s per run = ~11.5 hours single-threaded. With 4-core multiprocessing = ~3 hours. Manageable.

### Files to Create

```
backend/
  engine/
    data_utils.py              # Clean data loading with resampling
  optimizer/
    __init__.py
    sweep.py                   # SweepEngine class
    scoring.py                 # Sharpe calculation, composite scoring
    experiment_tracker.py      # ExperimentTracker — central results store
    run_sweep.py               # CLI entry point
```

### Output

Results flow into the `experiments` table via `ExperimentTracker`. After a sweep completes, the tracker can surface the top candidates:

```python
tracker = ExperimentTracker()
top = tracker.get_top_candidates(n=10)
# Returns ranked list with scores, ready for Phase 2 validation
```

No markdown files generated. No manual analysis needed. The data stays in SQLite where the system can query it programmatically.

---

## Phase 2: Validation Framework

### What It Does

Takes the top candidates from Phase 1 and stress-tests them for overfitting. This is the most important phase — without it, everything that follows is finding noise.

### Why This Must Come Before Phase 3

Phase 3 (composable strategies) will generate thousands of combinations. Without overfitting controls, it WILL find combinations that look great on historical data but fail live. The validation framework acts as a filter that every candidate must pass.

### The Overfitting Problem

With 82,944 backtest combinations, pure chance guarantees that some will show strong returns. If you test 80,000 random strategies, roughly 400 will show >2% annual alpha at 95% confidence — by statistical chance alone.

The validation framework distinguishes real edges from lucky noise.

### Components

#### 1. Train/Test Split

**Fixed holdout**: Train on 2020-2023, test on 2024-2025.

```python
def validate_holdout(strategy_class, params, symbol, timeframe):
    """Run on training period, then independently on test period."""
    train_data = load_backtest_data(symbol, timeframe, "2020-01-01", "2023-12-31")
    test_data = load_backtest_data(symbol, timeframe, "2024-01-01", "2025-12-31")

    train_result = Backtester(train_data, strategy_class, params,
                              spread=0.0003).run()
    test_result = Backtester(test_data, strategy_class, params,
                             spread=0.0003).run()

    return {
        "train": train_result,
        "test": test_result,
        "degradation": train_result['return_pct'] - test_result['return_pct']
    }
```

A strategy that returns +5% in training but -2% in testing is overfit. We want strategies where test performance is close to training performance (low degradation).

#### 2. Walk-Forward Validation

More rigorous than a single split. Uses rolling windows:

```
Window 1: Train 2020-2021, Test 2022
Window 2: Train 2021-2022, Test 2023
Window 3: Train 2022-2023, Test 2024
Window 4: Train 2023-2024, Test 2025
```

A robust strategy should show positive test returns in most or all windows, not just one.

```python
def walk_forward(strategy_class, params, symbol, timeframe,
                 train_years=2, test_years=1, start=2020, end=2025):
    """Rolling walk-forward validation."""
    windows = []
    for year in range(start, end - train_years - test_years + 2):
        train_start = f"{year}-01-01"
        train_end = f"{year + train_years - 1}-12-31"
        test_start = f"{year + train_years}-01-01"
        test_end = f"{year + train_years + test_years - 1}-12-31"

        # Run train and test independently
        # ...
        windows.append({"train_return": ..., "test_return": ...})

    # Strategy passes if test_return > 0 in >= 3 of 4 windows
    pass_count = sum(1 for w in windows if w["test_return"] > 0)
    return {"windows": windows, "pass_rate": pass_count / len(windows)}
```

#### 3. Multi-Asset Consistency Check

A real edge should work across related assets, not just one symbol. If StochRSI with overbought=75 works on SPY but fails on QQQ and IWM, it's likely overfit to SPY-specific noise.

```python
def multi_asset_check(strategy_class, params, symbols, timeframe):
    """Strategy must show positive returns on majority of assets."""
    results = {}
    for symbol in symbols:
        params_copy = {**params, "symbol": symbol}
        data = load_backtest_data(symbol, timeframe, "2020-01-01", "2025-12-31")
        result = Backtester(data, strategy_class, params_copy, spread=0.0003).run()
        results[symbol] = result["return_pct"]

    positive_count = sum(1 for r in results.values() if r > 0)
    return {
        "results": results,
        "positive_rate": positive_count / len(symbols),
        "passes": positive_count >= len(symbols) * 0.6  # 60% must be positive
    }
```

#### 4. Disqualification Rules

Hard filters that reject candidates before detailed validation:

```python
DISQUALIFICATION_RULES = {
    "min_trades": 30,           # Too few trades = unreliable statistics
    "min_trades_per_year": 6,   # Must trade regularly, not just once
    "max_drawdown": 25.0,       # >25% drawdown = too risky
    "min_profit_factor": 1.1,   # Must win more than it loses
    "min_win_rate": 0.35,       # Below 35% win rate = likely broken
    "max_win_rate": 0.85,       # Above 85% = likely overfitting to noise
}
```

#### 5. Validation Pipeline

Combine everything into a single pipeline:

```python
def validate_candidate(strategy_class, params, symbol, timeframe):
    """Full validation pipeline. Returns pass/fail with details."""

    # Step 1: Disqualification check (fast)
    full_result = run_backtest(strategy_class, params, symbol, timeframe,
                               "2020-01-01", "2025-12-31")
    if not passes_disqualification(full_result):
        return {"status": "REJECTED", "reason": "disqualified", ...}

    # Step 2: Train/test holdout
    holdout = validate_holdout(strategy_class, params, symbol, timeframe)
    if holdout["test"]["return_pct"] < 0:
        return {"status": "REJECTED", "reason": "negative_out_of_sample", ...}

    # Step 3: Walk-forward
    wf = walk_forward(strategy_class, params, symbol, timeframe)
    if wf["pass_rate"] < 0.5:
        return {"status": "REJECTED", "reason": "walk_forward_failure", ...}

    # Step 4: Multi-asset (if applicable)
    related_symbols = get_related_symbols(symbol)  # e.g. SPY -> [QQQ, IWM, DIA]
    ma = multi_asset_check(strategy_class, params, related_symbols, timeframe)

    return {
        "status": "PASSED" if ma["passes"] else "MARGINAL",
        "full_result": full_result,
        "holdout": holdout,
        "walk_forward": wf,
        "multi_asset": ma,
    }
```

### Files to Create

```
backend/
  optimizer/
    validation.py          # Holdout, walk-forward, multi-asset checks
    disqualify.py          # Hard filter rules
    pipeline.py            # Full validation pipeline
```

### Output

Validation **updates the existing experiment row** in the `experiments` table:

```python
def validate_and_update(tracker, experiment_id, strategy_class, params, symbol, timeframe):
    result = validate_candidate(strategy_class, params, symbol, timeframe)

    tracker.update_validation(
        experiment_id=experiment_id,
        validation_status=result["status"],        # "passed", "marginal", "rejected"
        test_return_pct=result["holdout"]["test"]["return_pct"],
        validation_details=json.dumps({
            "holdout_degradation": result["holdout"]["degradation"],
            "walk_forward_pass_rate": result["walk_forward"]["pass_rate"],
            "multi_asset_positive_rate": result["multi_asset"]["positive_rate"],
            "rejection_reason": result.get("reason"),
        })
    )
```

No separate output. The experiments table is the single source of truth. Query `validation_status = 'passed'` to get real candidates.

---

## Phase 3: Composable Strategy Framework

### What It Does

Instead of only sweeping parameters on existing strategies, defines trading logic as composable building blocks — entry signals, exit rules, filters, and position sizing — then auto-generates and tests thousands of novel combinations.

### Why This Is Phase 3

Requires Phase 1 (sweep engine to test combinations) and Phase 2 (validation to avoid overfitting) as foundations. Without both, generating thousands of combinations produces noise.

### Design

A single `ComposableStrategy` class that inherits from `Strategy` and wires together pluggable components:

```python
class ComposableStrategy(Strategy):
    def __init__(self, data, events, parameters, initial_cash, broker):
        super().__init__(data, events, parameters, initial_cash, broker)
        self.symbol = parameters['symbol']

        # Components (passed via parameters)
        self.entry_signal = parameters['entry_signal']     # callable
        self.exit_signal = parameters['exit_signal']        # callable
        self.regime_filter = parameters.get('filter')       # callable or None
        self.position_sizer = parameters['position_sizer']  # callable
        self.stop_loss_fn = parameters['stop_loss_fn']      # callable

        # Pre-calculate all indicators on the full dataset
        self.indicators = parameters['indicator_fn'](data)

        # State
        self.bar_index = 0
        self.current_sl = None
```

### Building Blocks

#### Entry Signals

Each is a function: `(row, prev_row, indicators) -> 'long' | 'short' | None`

```python
# Threshold crossing
def stochrsi_cross_up(row, prev, ind):
    if prev['k'] <= 30 and row['k'] > 30:
        return 'long'

def stochrsi_cross_down(row, prev, ind):
    if prev['k'] >= 70 and row['k'] < 70:
        return 'short'

# Breakout
def donchian_breakout_long(row, prev, ind):
    if row['Close'] > row['entry_high']:
        return 'long'

# MACD
def macd_cross_bullish(row, prev, ind):
    if prev['macd_hist'] < 0 and row['macd_hist'] >= 0:
        return 'long'

# Bollinger touch
def bollinger_lower_touch(row, prev, ind):
    if row['Close'] <= row['bb_lower']:
        return 'long'
```

#### Filters (regime gates)

Each is a function: `(row, indicators) -> bool` (True = allow trade)

```python
def adx_ranging(row, ind):
    return row['adx'] < 25

def adx_trending(row, ind):
    return row['adx'] > 25

def chop_trending(row, ind):
    return row['chop'] < 38.2

def sma_uptrend(row, ind):
    return row['Close'] > row['sma_200']

def no_filter(row, ind):
    return True
```

#### Exit Rules

Each is a function: `(row, position_side, entry_price, indicators) -> bool`

```python
def opposite_signal(row, side, entry, ind):
    if side == 'long' and row['k'] > 80:
        return True
    if side == 'short' and row['k'] < 20:
        return True

def atr_trailing_stop(multiplier):
    def _exit(row, side, entry, ind):
        stop_dist = row['atr'] * multiplier
        if side == 'long' and row['Close'] < entry - stop_dist:
            return True
        if side == 'short' and row['Close'] > entry + stop_dist:
            return True
    return _exit

def donchian_exit(row, side, entry, ind):
    if side == 'long' and row['Close'] < row['exit_low']:
        return True
    if side == 'short' and row['Close'] > row['exit_high']:
        return True

def time_stop(max_bars):
    bars_held = [0]
    def _exit(row, side, entry, ind):
        bars_held[0] += 1
        return bars_held[0] >= max_bars
    return _exit
```

#### Position Sizing

```python
def fixed_pct(pct=0.25):
    def _size(equity, price, atr_val):
        return (equity * pct) / price
    return _size

def volatility_scaled(risk_pct=0.02, atr_mult=2.0):
    def _size(equity, price, atr_val):
        risk_amt = equity * risk_pct
        stop_dist = atr_val * atr_mult
        if stop_dist <= 0:
            return 0
        return min(risk_amt / stop_dist, (equity * 0.25) / price)
    return _size
```

#### Indicator Calculator

Pre-computes all indicators on the dataset. Only calculates what's needed:

```python
def make_indicator_fn(indicators_needed):
    """Returns a function that calculates requested indicators on a DataFrame."""
    def calculate(df):
        result = df.copy()
        if 'stochrsi' in indicators_needed:
            # Use iterative StochRSI for compatibility
            stoch = StochRSI(14, 14, 3, 3)
            k_vals, d_vals = [], []
            for p in df['Close']:
                stoch.update(p)
                k_vals.append(stoch.k if stoch.ready else None)
                d_vals.append(stoch.d if stoch.ready else None)
            result['k'] = k_vals
            result['d'] = d_vals
        if 'adx' in indicators_needed:
            result['adx'] = adx(df['High'], df['Low'], df['Close'], 14)
        if 'atr' in indicators_needed:
            result['atr'] = atr(df['High'], df['Low'], df['Close'], 14)
        if 'bollinger' in indicators_needed:
            bb = bollinger_bands(df['Close'], 20, 2.0)
            result['bb_upper'] = bb['upper']
            result['bb_lower'] = bb['lower']
            result['bb_middle'] = bb['middle']
        if 'macd' in indicators_needed:
            m = macd(df['Close'], 12, 26, 9)
            result['macd_hist'] = m['histogram']
        if 'donchian' in indicators_needed:
            ch = donchian_channels(df['High'], df['Low'], 20, 10)
            result['entry_high'] = ch['upper_entry']
            result['entry_low'] = ch['lower_entry']
            result['exit_high'] = ch['upper_exit']
            result['exit_low'] = ch['lower_exit']
        if 'sma_200' in indicators_needed:
            result['sma_200'] = sma(df['Close'], 200)
        if 'chop' in indicators_needed:
            result['chop'] = chop_index(df['High'], df['Low'], df['Close'], 14)
        return result
    return calculate
```

### Combination Generation

```python
ENTRY_SIGNALS = [
    stochrsi_cross_up, stochrsi_cross_down,
    donchian_breakout_long, donchian_breakout_short,
    macd_cross_bullish, macd_cross_bearish,
    bollinger_lower_touch, bollinger_upper_touch,
    rsi_oversold_reversal,
]

FILTERS = [
    no_filter, adx_ranging, adx_trending,
    chop_trending, sma_uptrend, sma_downtrend,
]

EXIT_RULES = [
    opposite_signal,
    atr_trailing_stop(2.0), atr_trailing_stop(3.0),
    donchian_exit,
    time_stop(20), time_stop(50),
]

POSITION_SIZERS = [
    fixed_pct(0.25),
    volatility_scaled(0.02, 2.0),
    volatility_scaled(0.02, 3.0),
]

# Total combinations: 9 * 6 * 6 * 3 = 972 per symbol/timeframe
# Across 8 symbols, 3 timeframes = 23,328 combinations
```

Each combination is a parameter dict passed to `ComposableStrategy` and run through the Phase 1 sweep engine + Phase 2 validation pipeline.

### Multi-Indicator Confluence

The framework naturally supports confluence by combining entry signals:

```python
def confluence(*signals):
    """All signals must agree for entry."""
    def _entry(row, prev, ind):
        results = [s(row, prev, ind) for s in signals]
        if all(r == 'long' for r in results if r is not None):
            return 'long'
        if all(r == 'short' for r in results if r is not None):
            return 'short'
        return None
    return _entry

# Example: StochRSI oversold reversal + ADX ranging + price above SMA200
# This is multi-indicator confluence — never tested manually
entry = confluence(stochrsi_cross_up, sma_uptrend_signal)
```

### Files to Create

```
backend/
  optimizer/
    composable_strategy.py   # ComposableStrategy class
    building_blocks.py       # Entry, exit, filter, sizing functions
    indicator_calculator.py  # Indicator pre-computation
    combination_generator.py # Cartesian product of building blocks
```

### Important Limitations

- Zone-based state machines (like StochRSI's oversold-zone tracking) are hard to express as pure functions. The composable framework starts with stateless condition checks and adds zone-based entries as a special case later.
- Indicator parameters (e.g. RSI period 7 vs 14) are fixed within the indicator calculator. Sweeping indicator parameters requires Phase 1's parameter grid ON TOP of Phase 3's combinations. This multiplies the search space — use Phase 2 validation aggressively.

---

## Phase 4: LLM Agent

### What It Does

Uses Claude to analyse backtest results, reason about WHY strategies succeed or fail, generate new strategy code (or modify existing), and iterate. This is the most creative layer — it can discover structural changes that parameter sweeps and composable blocks cannot.

### Why This Is Phase 4

Requires all three previous phases:
- Phase 1 (sweep engine) to test what the LLM generates
- Phase 2 (validation) to prevent the LLM from finding overfitted noise
- Phase 3 (composable framework) gives the LLM building blocks to work with, though it can also write raw strategy code

### Architecture

```
┌──────────────────────────────────────────────────────────┐
│                    LLM Agent Loop                         │
│                                                           │
│  1. Load experiment history (top/bottom performers)       │
│  2. Load indicator library docs + strategy examples       │
│  3. Prompt Claude:                                        │
│     "Here are the results. Why did X fail? What's next?"  │
│  4. Claude responds with:                                 │
│     - Analysis of why strategies failed                   │
│     - Hypothesis for a new approach                       │
│     - Complete strategy code (or param modifications)     │
│  5. Validate the generated code (syntax, imports, etc.)   │
│  6. Run through sweep engine (Phase 1)                    │
│  7. Run through validation pipeline (Phase 2)             │
│  8. Log results to experiment tracker                     │
│  9. Feed results back to step 1                           │
│  10. Repeat for N iterations                              │
└──────────────────────────────────────────────────────────┘
```

### Prompt Design

The prompt is built dynamically from the experiment tracker — not a static template. `ExperimentTracker.get_summary_for_llm()` provides the experiment history, replacing the old `research_insights.md`.

```python
# backend/optimizer/prompt_builder.py

def build_agent_prompt(tracker: ExperimentTracker, iteration: int):
    """Build the LLM prompt from live experiment data."""

    # 1. Pull context from experiment tracker (replaces research_insights.md)
    experiment_summary = tracker.get_summary_for_llm(max_results=20)

    # 2. Pull previous LLM iterations and their outcomes
    llm_history = tracker.get_experiments_by_source("llm_generated")

    # 3. Identify what hasn't been tried
    gaps = tracker.get_untested_combinations(...)

    return f"""
You are a quantitative strategy researcher. Your job is to design
trading strategies that produce alpha after realistic transaction costs.

## Backtest Settings (MANDATORY)
spread=0.0003 (0.03%), delay=0 (fill at bar Close). These match live
execution. Any strategy must be profitable AFTER these costs.

## Available Indicators
{INDICATOR_DOCS}

## Strategy Interface
{STRATEGY_BASE_CLASS}

## Experiment Results (from database — all using correct cost model)
{experiment_summary}

## Your Previous Iterations (LLM-generated strategies)
{format_llm_history(llm_history)}

## Untested Directions
{gaps}

## Your Task
Analyse the results above. Form a hypothesis about why the top
performers worked and the bottom performers failed. Then write a
complete strategy class that you believe will outperform.

The strategy MUST:
- Inherit from Strategy
- Implement on_data(self, index, row) and on_event(self, event)
- Use only indicators from the available library
- Include position sizing (max 25% equity per position)
- Include a stop loss mechanism

Output ONLY the Python code for the strategy class.
"""
```

The key difference from a static markdown file: this prompt is **regenerated each iteration** from the latest experiment data. As the system learns, the prompt evolves.

### Code Validation

Before running any LLM-generated code:

```python
def validate_generated_code(code_string):
    """Safety checks on LLM-generated strategy code."""
    # 1. Syntax check
    try:
        ast.parse(code_string)
    except SyntaxError as e:
        return False, f"Syntax error: {e}"

    # 2. Must inherit from Strategy
    if "Strategy" not in code_string:
        return False, "Must inherit from Strategy"

    # 3. Must implement required methods
    if "def on_data" not in code_string:
        return False, "Missing on_data method"

    # 4. No dangerous imports
    dangerous = ['os', 'sys', 'subprocess', 'shutil', 'socket']
    for d in dangerous:
        if f"import {d}" in code_string:
            return False, f"Dangerous import: {d}"

    # 5. No network calls
    if 'requests' in code_string or 'urllib' in code_string:
        return False, "Network access not allowed"

    return True, "OK"
```

### What the LLM Can Do That Sweeps Cannot

- **Structural changes**: "Donchian breakout loses in choppy markets → what if I gate it with CHOP index?" (requires combining two strategies in a novel way)
- **New indicators**: Implement OBV or VWAP from scratch when it hypothesises volume matters
- **Adaptive logic**: "Use tight stops in high-vol regimes, wide stops in low-vol" (hard to express in a parameter grid)
- **Cross-strategy learning**: "Mean reversion entries with trend-following exits" (combining patterns from different strategy families)
- **Market microstructure reasoning**: "This strategy trades too frequently on 5m — the spread eats the edge. What if I only trade when ATR > 2x average?"

### Files to Create

```
backend/
  optimizer/
    llm_agent.py           # Main agent loop
    prompt_builder.py      # Construct prompts from experiment history
    code_validator.py      # Syntax and safety checks
    strategy_registry.py   # Dynamic strategy registration
```

### Iteration Protocol

```python
def run_llm_iteration(tracker, iteration_num):
    # 1. Build prompt from experiment tracker (replaces reading markdown)
    prompt = build_agent_prompt(tracker, iteration_num)

    # 2. Call Claude API
    code = call_claude(prompt)

    # 3. Validate generated code
    valid, reason = validate_generated_code(code)
    if not valid:
        tracker.save_llm_failure(iteration_num, reason, code)
        return

    # 4. Write strategy file, register in STRATEGY_MAP
    strategy_class = register_strategy(code, f"LLMStrategy_v{iteration_num}")

    # 5. Sweep across target assets (Phase 1)
    sweep = SweepEngine()
    for symbol in TARGET_SYMBOLS:
        for tf in TARGET_TIMEFRAMES:
            sweep.run_sweep(strategy_class, {}, symbol, tf,
                           "2020-01-01", "2025-12-31")

    # 6. Save results to experiment tracker
    experiment_id = f"llm_iter_{iteration_num:03d}"
    for result in sweep.results:
        tracker.save(experiment_id, ..., strategy_source="llm_generated",
                     parent_experiment_id=f"llm_iter_{iteration_num-1:03d}")

    # 7. Validate top results (Phase 2)
    top = tracker.get_top_candidates(n=5, experiment_id=experiment_id)
    for candidate in top:
        validate_and_update(tracker, candidate['id'], ...)

    # 8. Results automatically available in next iteration's prompt
    #    via tracker.get_summary_for_llm()
```

The `parent_experiment_id` field tracks lineage — you can trace what each iteration was building on.

### Safety

- Generated code runs in the same process as the backtester (no sandbox). The code validation step is critical.
- Each iteration is logged with the full prompt, generated code, and results in the experiments table. This creates an audit trail.
- The agent never has access to live trading — only the backtester with PaperTrader.
- Generated strategy files are written to `backend/strategies/generated/` to keep them separate from hand-written strategies.

---

## Implementation Order & Dependencies

```
Phase 0: Data Strategy (prerequisite)
└── experiments table in research.db (new, clean, separate from test_runs)
└── ExperimentTracker class (replaces analyze_results.py + research_insights.md)

Phase 1: Sweep Engine (~1 day)
├── Depends on: Phase 0 (experiment tracker)
├── data_utils.py (extract resampling logic from runner.py)
├── sweep.py (core engine — fetch once, backtest many)
├── scoring.py (Sharpe calculation, composite scoring)
├── experiment_tracker.py (query interface, LLM summary builder)
└── run_sweep.py (CLI entry point)

Phase 2: Validation Framework (~1 day)
├── Depends on: Phase 1 (sweep engine)
├── validation.py (holdout, walk-forward, multi-asset)
├── disqualify.py (hard filters)
└── pipeline.py (validates candidates, updates experiments table)

Phase 3: Composable Strategies (~2-3 days)
├── Depends on: Phase 1 + Phase 2
├── composable_strategy.py (generic Strategy subclass)
├── building_blocks.py (entry/exit/filter/sizing functions)
├── indicator_calculator.py (pre-computation)
└── combination_generator.py (cartesian product → sweep engine)

Phase 4: LLM Agent (~2-3 days)
├── Depends on: Phase 1 + Phase 2 (Phase 3 optional but helpful)
├── llm_agent.py (main loop — iterate, generate, test, learn)
├── prompt_builder.py (builds prompts from ExperimentTracker, not markdown)
├── code_validator.py (syntax and safety checks)
└── strategy_registry.py (dynamic strategy registration)
```

**Critical path**: Phase 0 → Phase 1 → Phase 2 → (Phase 3 | Phase 4 in parallel).

Phase 3 and Phase 4 can be built independently once Phase 1 + 2 are complete. Phase 4 benefits from Phase 3's building blocks but doesn't require them.

### Complete File Map

```
backend/
  engine/
    data_utils.py                   # NEW — clean data loading with resampling
    backtester.py                   # EXISTING — no changes needed
    strategy.py                     # EXISTING — no changes needed
    paper_trader.py                 # EXISTING — no changes needed
    alpaca_loader.py                # EXISTING — no changes needed
  optimizer/                        # NEW directory
    __init__.py
    experiment_tracker.py           # Central results store + query interface
    sweep.py                        # SweepEngine class
    scoring.py                      # Sharpe, composite scoring
    validation.py                   # Holdout, walk-forward, multi-asset
    disqualify.py                   # Hard filter rules
    pipeline.py                     # Full validation pipeline
    composable_strategy.py          # Generic composable Strategy subclass
    building_blocks.py              # Entry/exit/filter/sizing functions
    indicator_calculator.py         # Indicator pre-computation
    combination_generator.py        # Cartesian product generator
    llm_agent.py                    # LLM iteration loop
    prompt_builder.py               # Dynamic prompt construction
    code_validator.py               # Syntax and safety checks
    strategy_registry.py            # Dynamic STRATEGY_MAP registration
    run_sweep.py                    # CLI: python -m backend.optimizer.run_sweep
  strategies/
    generated/                      # NEW directory — LLM-generated strategies go here
  database.py                       # EXISTING — add experiments table migration
  analyze_results.py                # EXISTING — retired, not used by new system
```

---

## Data Flow Summary

```
                    ┌─────────────────────┐
                    │  experiments table   │ ← Single source of truth
                    │  (SQLite)            │
                    └──────┬──────────────┘
                           │
              ┌────────────┼────────────────┐
              │            │                │
         ┌────▼────┐  ┌───▼────┐     ┌─────▼──────┐
         │ Phase 1 │  │Phase 2 │     │  Phase 4   │
         │ Sweep   │  │Validate│     │  LLM Agent │
         │ Engine  │  │Pipeline│     │            │
         └────┬────┘  └───┬────┘     └─────┬──────┘
              │            │                │
         writes        updates          reads summary
         results     validation_status  writes new results
              │            │                │
              └────────────┼────────────────┘
                           │
                           ▼
                  tracker.get_top_candidates()
                  tracker.get_summary_for_llm()
                  tracker.get_failures_for_strategy()
```

**No markdown files in the loop.** The system reads and writes SQLite. You check in by asking for `tracker.get_top_candidates()` — a few lines, not a 400-line report.

---

## What Happens to Existing Files

| File | Status | Reason |
|---|---|---|
| `backend/database.py` | **Extended** | Add `experiments` table migration to `initialize_db()` |
| `backend/analyze_results.py` | **Retired** | Replaced by `ExperimentTracker`. Not deleted, just unused. |
| `.agent/memory/research_insights.md` | **Retired** | Was auto-generated from contaminated data. No longer updated. |
| `backend/research.db` | **Kept** | Same file, new `experiments` table added alongside existing tables |
| `test_runs` table | **Untouched** | Legacy data stays. New system uses `experiments` table only. |
| `insights` table | **Untouched** | Legacy. Not read by new system. |
| `live_trade_log` table | **Kept** | Still used by live trading infrastructure (separate concern). |

---

## Risk: Overfitting Is the Primary Threat

This entire system is an overfitting machine if not handled carefully. The more combinations you test, the more likely you are to find spurious edges.

**Safeguards built into the design:**
1. **Phase 2 comes before Phase 3** — validation is not optional
2. **Walk-forward validation** — tests on data the strategy has never seen
3. **Multi-asset consistency** — real edges generalise, noise doesn't
4. **Minimum trade count** — rejects strategies with too few trades to be statistically meaningful
5. **Sharpe over raw returns** — penalises inconsistent strategies
6. **Complexity awareness** — fewer parameters = more likely to be real
7. **Clean data only** — new system never reads contaminated delay=1 results

**The ultimate test**: Any strategy that passes all validation should be forward-tested on the existing cloud infrastructure (live paper trading) before committing real capital. The infrastructure for this already exists and is proven.

---

## Quick Reference: Key Existing Files

| File | Role |
|---|---|
| `backend/engine/backtester.py` | Backtester class — the core execution engine |
| `backend/engine/strategy.py` | Strategy ABC — interface all strategies implement |
| `backend/engine/paper_trader.py` | PaperTrader broker — simulated execution with spread |
| `backend/engine/broker_adapter.py` | BrokerAdapter ABC — broker interface contract |
| `backend/engine/alpaca_loader.py` | Alpaca data fetching + timeframe handling |
| `backend/runner.py` | CLI entry, STRATEGY_MAP, run_backtest, run_matrix |
| `backend/database.py` | DatabaseManager — extends to include experiments table |
| `backend/indicators/*.py` | 9 indicator files (vectorized + stateful) |
| `backend/strategies/*.py` | 22 strategy files (templates for LLM agent) |
| `backend/analyze_results.py` | RETIRED — was research_insights.md generator |

### Backtest Settings (hardcoded in sweep engine)

```python
SPREAD = 0.0003       # 0.03% — validated against live execution
EXECUTION_DELAY = 0   # Fill at bar Close — matches live (delay=1 is BROKEN)
INITIAL_CAPITAL = 10000.0
```

These are never configurable in the sweep engine. Every experiment uses the same cost model. No exceptions.

---

*Created: 2026-02-10*
*Status: Design document — not yet implemented*
