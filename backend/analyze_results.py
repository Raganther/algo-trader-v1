import json
import pandas as pd
import numpy as np
import os
import sqlite3
from datetime import datetime


MARKDOWN_FILE = '.agent/memory/research_insights.md'

class InsightManager:
    def __init__(self):
        from backend.database import DatabaseManager
        self.db = DatabaseManager()
        self.db.initialize_db() # Ensure table exists
        self.insights = self._load_insights()

    def _load_insights(self):
        return self.db.get_all_insights()

    def save_insights(self):
        for insight in self.insights:
            self.db.save_insight(insight)
        self._generate_markdown()

    def add_insight(self, insight_type, description, confidence, scope, parameters=None, expiration=None):
        # Check for duplicates based on Type and Scope
        # We need to be careful about scope comparison (list vs json string)
        # The loaded insights have scope as list (handled in get_all_insights)
        
        for existing in self.insights:
            # Compare scope sets to be order-independent if needed, or just direct list comparison
            if existing['type'] == insight_type and existing['scope'] == scope:
                # Update existing
                existing['description'] = description
                existing['confidence'] = confidence
                existing['parameters'] = parameters # Update params
                existing['last_updated'] = datetime.now().isoformat()
                print(f"ðŸ”„ Updated Insight: {existing['insight_id']}")
                return

        # Create new ID
        new_id = f"INSIGHT-{len(self.insights) + 1:03d}"
        
        new_insight = {
            "insight_id": new_id,
            "type": insight_type,
            "description": description,
            "confidence": confidence,
            "scope": scope,
            "parameters": parameters,
            "expiration": expiration,
            "status": "active",
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat()
        }
        self.insights.append(new_insight)
        print(f"âœ¨ New Insight Generated: {new_id}")

    def _generate_markdown(self):
        """Auto-generates the human-readable markdown report (Hybrid Layout)."""
        content = "# ðŸ§  Research Insights (Strategy Profile)\n\n"
        content += "> [!NOTE]\n"
        content += "> This file is automatically generated by the Analysis Engine. Do not edit manually.\n\n"
        
        # 1. Group Insights by Strategy & Symbol
        grouped_insights = {}
        for i in self.insights:
            params = i.get('parameters', {})
            strategy = params.get('strategy')
            symbol = params.get('symbol')
            
            if not strategy or not symbol:
                continue 
                
            key = (strategy, symbol)
            if key not in grouped_insights:
                grouped_insights[key] = []
            grouped_insights[key].append(i)
            
        # 2. Sort Groups
        sorted_keys = sorted(grouped_insights.keys(), key=lambda k: (
            max([x['confidence'] for x in grouped_insights[k]]),
            max([x.get('parameters', {}).get('avg_annual_return', x.get('parameters', {}).get('return_pct', -999)) for x in grouped_insights[k]])
        ), reverse=True)
        
        for i, (strategy, symbol) in enumerate(sorted_keys):
            group = grouped_insights[(strategy, symbol)]
            
            # Determine Status
            max_conf = max([x['confidence'] for x in group])
            status_icon = "ðŸŸ¢" if max_conf >= 0.8 else "ðŸŸ¡" if max_conf >= 0.5 else "ðŸ”´"
            status_text = "Verified Strategy" if max_conf >= 0.8 else "Watchlist Candidate" if max_conf >= 0.5 else "Under Review"
            
            # --- STRATEGY PROFILE CALCULATION ---
            # 1. Fetch all runs for this strategy/symbol to build the profile
            # We need to access the DB to get the runs and equity curve.
            # Since InsightManager has self.db, we can use it.
            
            # Get all runs for this combo (we need to query DB or filter from loaded runs if we had them)
            # analyze() loads runs but doesn't pass them to InsightManager. 
            # We should probably fetch them here or pass them in.
            # Let's fetch them here for simplicity.
            
            # We need the timeframe too. Usually insights are grouped by Strategy/Symbol, 
            # but runs might have different timeframes. We should pick the "Best" timeframe or aggregate?
            # The user wants a "Strategy Profile". Usually a strategy is defined by (Strategy, Symbol, Timeframe).
            # Let's assume the "Best" timeframe from the insights is the primary one.
            
            # --- STRATEGY PROFILE CALCULATION ---
            # 1. Fetch all runs for this strategy/symbol to build the profile
            # We need to access the DB to get the runs and equity curve.
            # Since InsightManager has self.db, we can use it.
            
            # Get all runs for this combo (we need to query DB or filter from loaded runs if we had them)
            # analyze() loads runs but doesn't pass them to InsightManager. 
            # We should probably fetch them here or pass them in.
            # Let's fetch them here for simplicity.
            
            # We need the timeframe too. Usually insights are grouped by Strategy/Symbol, 
            # but runs might have different timeframes. We should pick the "Best" timeframe or aggregate?
            # The user wants a "Strategy Profile". Usually a strategy is defined by (Strategy, Symbol, Timeframe).
            # Let's assume the "Best" timeframe from the insights is the primary one.
            
            best_insight = sorted(group, key=lambda x: x.get('parameters', {}).get('return_pct', -999), reverse=True)[0]
            best_params = best_insight.get('parameters', {})
            timeframe = best_params.get('timeframe', '1h')
            
            # Fetch Iteration Index from DB for the best run
            # We need to find the run corresponding to this insight
            # The insight parameters usually contain enough info, but maybe not iteration_index if it's old.
            # Let's query the DB for the run with matching return/timeframe/year to find the iteration index.
            # OR, we can just rely on the fact that we will fetch all runs below and can find the "Best" one there.
            
            # Fetch Yearly Runs for Table
            # We need to query test_runs for this combo
            conn = self.db.get_connection()
            conn.row_factory = sqlite3.Row
            cursor = conn.cursor()
            cursor.execute('''
                SELECT * FROM test_runs 
                WHERE strategy = ? AND symbol = ?
                ORDER BY start_date DESC
            ''', (strategy, symbol))
            runs = [dict(row) for row in cursor.fetchall()]
            conn.close()
            
            # Parse parameters JSON
            for r in runs:
                if isinstance(r.get('parameters'), str):
                    try:
                        r['parameters'] = json.loads(r['parameters'])
                    except:
                        r['parameters'] = {}
            
            # Find the "Best Iteration" (Champion)
            # We group runs by iteration_index
            iteration_groups = {}
            for r in runs:
                idx = r.get('iteration_index', 0)
                if idx not in iteration_groups: iteration_groups[idx] = []
                iteration_groups[idx].append(r)
            
            # Calculate total return for each iteration to find the winner
            best_iteration_idx = 0
            best_iteration_return = -9999
            
            for idx, iter_runs in iteration_groups.items():
                # Calculate cumulative return for this iteration
                cum_ret = 1.0
                for r in iter_runs:
                    cum_ret *= (1 + r['return_pct']/100)
                total_ret = (cum_ret - 1) * 100
                
                if total_ret > best_iteration_return:
                    best_iteration_return = total_ret
                    best_iteration_idx = idx
            
            # Use the Best Iteration for the Profile
            profile_runs = iteration_groups.get(best_iteration_idx, [])
            
            # Fetch Composite Curve (We need to filter by iteration index now!)
            # The current get_composite_equity_curve fetches ALL runs.
            # We need to update it or filter it.
            # For now, let's assume get_composite_equity_curve stitches EVERYTHING, which is WRONG if we have multiple iterations.
            # We need to stitch only the runs for the BEST iteration.
            # Let's manually stitch here since we have the runs.
            
            composite_curve = []
            last_equity = None
            
            # Sort profile runs by date
            profile_runs.sort(key=lambda x: x['start_date'])
            
            for run in profile_runs:
                curve_data = self.db.get_equity_curve(run['test_id'])
                if not curve_data: continue
                
                # Normalize
                current_start_equity = curve_data[0]['equity']
                if last_equity is not None and current_start_equity > 0:
                    adjustment_factor = last_equity / current_start_equity
                    for point in curve_data:
                        point['equity'] = point['equity'] * adjustment_factor
                
                composite_curve.extend(curve_data)
                last_equity = composite_curve[-1]['equity']
            
            # Calculate Risk Metrics from Curve
            global_max_dd = 0.0
            time_in_dd = 0.0
            longest_dd_duration = 0 # Days
            
            if composite_curve:
                df_curve = pd.DataFrame(composite_curve)
                if not df_curve.empty:
                    df_curve['equity'] = pd.to_numeric(df_curve['equity'])
                    if 'time' in df_curve.columns:
                        # Handle timestamp conversion
                        if isinstance(df_curve.iloc[0]['time'], int):
                            df_curve['datetime'] = pd.to_datetime(df_curve['time'], unit='s')
                        else:
                            df_curve['datetime'] = pd.to_datetime(df_curve['time'])
                        df_curve.set_index('datetime', inplace=True)
                    
                    # Calculate DD
                    # Ensure unique index to avoid ambiguity
                    df_curve = df_curve[~df_curve.index.duplicated(keep='first')]
                    
                    hwm = df_curve['equity'].cummax()
                    dd = (hwm - df_curve['equity']) / hwm
                    global_max_dd = dd.max() * 100
                    
                    # Time in Drawdown
                    # Count points where dd > 0 (or > epsilon)
                    underwater = dd > 0.0001
                    time_in_dd = (underwater.sum() / len(underwater)) * 100
                    
                    # Longest DD Duration
                    max_duration = pd.Timedelta(0)
                    last_peak_time = df_curve.index[0]
                    
                    for idx, row in df_curve.iterrows():
                        if row['equity'] >= hwm.loc[idx]:
                            duration = idx - last_peak_time
                            if duration > max_duration:
                                max_duration = duration
                            last_peak_time = idx
                    
                    final_duration = df_curve.index[-1] - last_peak_time
                    if final_duration > max_duration:
                        max_duration = final_duration
                        
                    longest_dd_duration = max_duration.days

            # Calculate Aggregates for Profile
            total_return = 1.0
            years_tested = []
            avg_win_rate = 0.0
            
            for r in profile_runs:
                ret = r['return_pct']
                total_return *= (1 + ret/100)
                y = r.get('start_date', '').split('-')[0]
                if y: years_tested.append(y)
                avg_win_rate += r['win_rate']
                
            total_return_pct = (total_return - 1) * 100
            if profile_runs:
                avg_win_rate /= len(profile_runs)
                avg_annual_return = sum([r['return_pct'] for r in profile_runs]) / len(profile_runs)
            else:
                avg_annual_return = 0
            
            # Stability Score
            stability_score = "N/A"
            stability_label = "N/A"
            if len(profile_runs) >= 2:
                returns = [r['return_pct'] for r in profile_runs]
                std_dev = np.std(returns)
                if std_dev > 0:
                    ratio = avg_annual_return / std_dev
                    stability_score = f"{ratio:.1f}"
                    if ratio > 2.0: stability_label = "High"
                    elif ratio > 1.0: stability_label = "Moderate"
                    else: stability_label = "Low"
            
            # Best Config String
            # Use params from the first run of the best iteration
            best_config_str = "{}"
            if profile_runs:
                p = profile_runs[0].get('parameters', {})
                
                # Merge with defaults if Strategy is StochRSIMeanReversion
                if strategy == "StochRSIMeanReversion":
                    defaults = {
                        "rsi_period": 14,
                        "stoch_period": 14,
                        "k_period": 3,
                        "d_period": 3,
                        "overbought": 80,
                        "oversold": 20,
                        "adx_period": 14,
                        "adx_threshold": 25,
                        "stop_loss_atr": 2.0,
                        "atr_period": 14
                    }
                    # Update defaults with actual params (overrides)
                    defaults.update(p)
                    p = defaults
                
                # Filter out metadata
                clean_p = {k:v for k,v in p.items() if k not in ['strategy', 'symbol', 'return_pct', 'win_rate', 'max_drawdown', 'total_trades', 'timeframe', 'year']}
                best_config_str = json.dumps(clean_p)

            # --- GENERATE CONTENT ---
            content += f"## {i+1}. {strategy} on {symbol}\n"
            content += f"> **Status**: {status_icon} {status_text} | **Stability**: {stability_label} (Score: {stability_score})\n\n"
            
            # --- REALITY CHECK (FORWARD TEST) ---
            # Check if we have a matching live session for the BEST iteration
            matching_session = next((s for s in self.live_sessions 
                                     if s['strategy'] == strategy 
                                     and s['symbol'] == symbol 
                                     and s['iteration_index'] == best_iteration_idx), None)
            
            if matching_session:
                # Calculate Reality Gap
                theory_wr = avg_win_rate * 100
                reality_wr = matching_session['win_rate']
                wr_delta = reality_wr - theory_wr
                
                delta_icon = "ðŸŸ¢" if wr_delta >= -5 else "ðŸŸ¡" if wr_delta >= -15 else "ðŸ”´"
                
                content += f"### ðŸ§ª Reality Check (Forward Test)\n"
                content += f"> **Cumulative Reality**: {matching_session['session_count']} Sessions | **Gap**: {delta_icon} {wr_delta:+.1f}% (Win Rate)\n\n"
                
                content += f"| Metric | Theory (Backtest) | Reality (Live) | Delta |\n"
                content += f"| :--- | :--- | :--- | :--- |\n"
                content += f"| **Win Rate** | **{theory_wr:.1f}%** | **{reality_wr:.1f}%** | {wr_delta:+.1f}% |\n"
                content += f"| **Return** | {avg_annual_return:.2f}% (Avg Yr) | {matching_session['return_pct']:.2f}% (Realized) | N/A |\n"
                content += f"| **Trades** | {len(profile_runs)} Years | {matching_session['total_trades']} Trades | -\n\n"
            
            content += f"### ðŸ“Š Strategy Profile (Iteration {best_iteration_idx})\n"
            content += f"| Metric | Value | Notes |\n"
            content += f"| :--- | :--- | :--- |\n"
            content += f"| **Total Return** | **{total_return_pct:.2f}%** | Cumulative ({len(profile_runs)} Years) |\n"
            content += f"| **Avg Annual** | **{avg_annual_return:.2f}%** | Mean of yearly returns |\n"
            content += f"| **Win Rate** | **{avg_win_rate*100:.1f}%** | Avg across years |\n"
            content += f"| **Max Drawdown** | **{global_max_dd:.2f}%** | Global Worst Case |\n"
            content += f"| **Time in DD** | **{time_in_dd:.0f}%** | % Time Underwater |\n"
            content += f"| **Timeframe** | **{profile_runs[0]['timeframe']}** | Candle Size |\n"
            content += f"| **Best Config** | `{best_config_str}` | Winning Parameters |\n\n"
            
            content += "### ðŸ“‰ Risk Analysis\n"
            content += f"- **Longest Dry Spell**: {longest_dd_duration} Days\n"
            if profile_runs:
                worst_year = min(profile_runs, key=lambda x: x['return_pct'])
                content += f"- **Worst Year**: {worst_year['start_date'].split('-')[0]} ({worst_year['return_pct']:.2f}%)\n"
            content += "\n"
            
            content += "### ðŸ—“ï¸ Yearly Breakdown\n"
            content += "| Year | Return | Win Rate | DD | Trades |\n"
            content += "| :--- | :--- | :--- | :--- | :--- |\n"
            
            # Sort profile runs descending by date for table
            for r in sorted(profile_runs, key=lambda x: x['start_date'], reverse=True):
                y = r.get('start_date', '').split('-')[0]
                content += f"| {y} | {r['return_pct']:.2f}% | {r['win_rate']*100:.1f}% | {r['max_drawdown']:.2f}% | {r['total_trades']} |\n"
            content += "\n"
            
            # C. Iteration History (Table)
            # Show other iterations (aggregated or single best run per iteration?)
            # Let's show the "Best Run" of each OTHER iteration.
            
            other_iterations = []
            for idx, iter_runs in iteration_groups.items():
                if idx == best_iteration_idx: continue
                
                # Calculate metrics for this iteration
                cum_ret = 1.0
                for r in iter_runs: cum_ret *= (1 + r['return_pct']/100)
                tot_ret = (cum_ret - 1) * 100
                
                # Get params
                p = iter_runs[0].get('parameters', {})
                clean_p = {k:v for k,v in p.items() if k not in ['strategy', 'symbol', 'return_pct', 'win_rate', 'max_drawdown', 'total_trades', 'timeframe', 'year']}
                
                # Check for Reality Data for this iteration
                iter_session = next((s for s in self.live_sessions 
                                     if s['strategy'] == strategy 
                                     and s['symbol'] == symbol 
                                     and s['iteration_index'] == idx), None)
                
                reality_badge = "-"
                if iter_session:
                    # Calculate Win Rate Delta
                    iter_theory_wr = np.mean([r['win_rate'] for r in iter_runs]) * 100
                    iter_reality_wr = iter_session['win_rate']
                    iter_delta = iter_reality_wr - iter_theory_wr
                    icon = "ðŸŸ¢" if iter_delta >= -5 else "ðŸ”´"
                    reality_badge = f"{icon} {iter_delta:+.1f}%"

                other_iterations.append({
                    'iteration': idx,
                    'return': tot_ret,
                    'params': json.dumps(clean_p),
                    'timeframe': iter_runs[0]['timeframe'],
                    'reality': reality_badge
                })
            
            if other_iterations:
                content += "### ðŸ“œ Iteration History (Variations)\n"
                content += "| Iter | TF | Return | Reality | Params |\n"
                content += "| :--- | :--- | :--- | :--- | :--- |\n"
                other_iterations.sort(key=lambda x: x['return'], reverse=True)
                for r in other_iterations:
                    content += f"| **{r['iteration']}** | {r['timeframe']} | **{r['return']:.2f}%** | {r['reality']} | `{r['params']}` |\n"
                content += "\n"
                
            content += "---\n\n"
        
        with open(MARKDOWN_FILE, 'w') as f:
            f.write(content)
        print(f"ðŸ“ Report updated: {MARKDOWN_FILE}")

def get_live_sessions():
    """Aggregates live trade logs into cumulative metrics by Iteration."""
    from backend.database import DatabaseManager
    db = DatabaseManager()
    trades = db.get_live_trades()
    
    if not trades:
        return []
        
    df = pd.DataFrame(trades)
    
    # Ensure iteration_index exists (fill NaN with 0 for legacy trades)
    if 'iteration_index' not in df.columns:
        df['iteration_index'] = 0
    df['iteration_index'] = df['iteration_index'].fillna(0).astype(int)
    
    # Group by Strategy + Symbol + Iteration (Cumulative)
    sessions = []
    grouped = df.groupby(['strategy', 'symbol', 'iteration_index'])
    
    for (strategy, symbol, iteration_index), group in grouped:
        if group.empty: continue
        
        # Calculate Metrics (FIFO PnL)
        realized_pnl = 0.0
        inventory = [] 
        wins = 0
        closed_trades = 0
        
        group = group.sort_values('timestamp')
        
        for _, trade in group.iterrows():
            qty = float(trade['qty'])
            price = float(trade['fill_price'])
            side = trade['side'].lower()
            
            if side == 'buy':
                inventory.append({'qty': qty, 'price': price})
            elif side == 'sell':
                remaining_qty_to_close = qty
                trade_pnl = 0.0
                
                while remaining_qty_to_close > 0 and inventory:
                    position = inventory[0]
                    
                    if position['qty'] > remaining_qty_to_close:
                        chunk_pnl = (price - position['price']) * remaining_qty_to_close
                        trade_pnl += chunk_pnl
                        position['qty'] -= remaining_qty_to_close
                        remaining_qty_to_close = 0
                    else:
                        chunk_pnl = (price - position['price']) * position['qty']
                        trade_pnl += chunk_pnl
                        remaining_qty_to_close -= position['qty']
                        inventory.pop(0)
                
                realized_pnl += trade_pnl
                closed_trades += 1
                if trade_pnl > 0:
                    wins += 1
                    
        total_pnl = realized_pnl
        total_trades = len(group) # Includes opens and closes
        # Win Rate based on CLOSED trades
        win_rate = (wins / closed_trades * 100) if closed_trades > 0 else 0.0
        
        # Estimate Return % (Assuming $100k paper account)
        initial_capital = 100000.0 
        return_pct = (total_pnl / initial_capital) * 100
        
        # Metadata
        first_trade = group.iloc[0]
        last_trade = group.iloc[-1]
        
        # Count unique sessions involved
        unique_sessions = group['session_id'].nunique()
        
        sessions.append({
            'strategy': strategy,
            'symbol': symbol,
            'iteration_index': iteration_index,
            'start_time': first_trade['timestamp'], # First trade ever
            'last_active': last_trade['timestamp'],
            'return_pct': return_pct,
            'win_rate': win_rate,
            'total_trades': total_trades,
            'closed_trades': closed_trades,
            'pnl': total_pnl,
            'session_count': unique_sessions
        })
        
    return sessions

def analyze(write=False):
    # Load Results from Database
    from backend.database import DatabaseManager
    db = DatabaseManager()
    try:
        data = db.get_all_test_runs()
        print(f"Loaded {len(data)} test runs from database.")
    except Exception as e:
        print(f"Error loading database: {e}")
        return

    # Convert to DataFrame
    records = []
    for entry in data:
        # Handle both JSON (nested metrics) and SQLite (flat) structures
        if 'metrics' in entry:
            metrics = entry['metrics']
            # Ensure we also get metadata from entry if not in metrics
            strategy = entry.get('strategy', 'Unknown')
            symbol = entry.get('symbol')
            timeframe = entry.get('timeframe')
            year = entry.get('year')
            params = entry.get('parameters', {})
        else:
            # Flat structure (SQLite)
            metrics = entry
            strategy = entry.get('strategy', 'Unknown')
            symbol = entry.get('symbol')
            timeframe = entry.get('timeframe')
            start_date = entry.get('start_date', '')
            year = entry.get('year')
            if not year and start_date:
                year = start_date.split('-')[0]
            params = entry.get('parameters', {})

        records.append({
            'strategy': strategy,
            'symbol': symbol,
            'timeframe': timeframe,
            'year': year,
            'return_pct': metrics.get('return_pct', 0),
            'win_rate': metrics.get('win_rate', 0),
            'drawdown': metrics.get('max_drawdown', 0),
            'trades': metrics.get('total_trades', 0),
            'parameters': params
        })
    
    df = pd.DataFrame(records)
    if df.empty:
        print("No results found.")
        return

    print(f"Loaded {len(df)} test runs.")
    print(f"Loaded {len(df)} test runs.")
    
    # Load Live Sessions
    live_sessions = get_live_sessions()
    print(f"Loaded {len(live_sessions)} live sessions.")
    
    manager = InsightManager()
    
    # Pass live sessions to manager for correlation
    manager.live_sessions = live_sessions

    # --- 1. Outlier Detection (Z-Score) ---
    mean_return = df['return_pct'].mean()
    std_return = df['return_pct'].std()
    df['z_score'] = (df['return_pct'] - mean_return) / std_return
    
    # Helper to prepare params with metrics
    def prepare_params(row):
        p = row['parameters'].copy() if isinstance(row['parameters'], dict) else {}
        p['return_pct'] = row['return_pct']
        p['win_rate'] = row['win_rate']
        p['max_drawdown'] = row['drawdown']
        p['total_trades'] = row['trades']
        p['timeframe'] = row['timeframe']
        p['year'] = row['year']
        p['strategy'] = row['strategy']
        p['symbol'] = row['symbol']
        return p

    # Detect Positive Anomalies (Winners)
    winners = df[df['z_score'] > 1.5].sort_values('return_pct', ascending=False)
    for _, row in winners.iterrows():
        params = prepare_params(row)
        if row['symbol'] == 'PORTFOLIO':
            desc = f"Meta-Strategy Validation: {row['strategy']} Portfolio returned {row['return_pct']:.2f}% (Z={row['z_score']:.2f})"
            manager.add_insight("meta_strategy_validation", desc, 0.95, ["PORTFOLIO", row['timeframe']], parameters=params)
        else:
            desc = f"Statistical Anomaly: {row['strategy']} on {row['symbol']} ({row['timeframe']}) returned {row['return_pct']:.2f}% (Z={row['z_score']:.2f})"
            manager.add_insight("statistical_anomaly", desc, 0.8, [str(row['symbol']), str(row['timeframe'])], parameters=params)

    # Detect Watchlist Candidates (Good but not Great: 0.5 < Z < 1.5)
    watchlist = df[(df['z_score'] > 0.5) & (df['z_score'] <= 1.5)].sort_values('return_pct', ascending=False)
    for _, row in watchlist.iterrows():
        params = prepare_params(row)
        if row['symbol'] != 'PORTFOLIO':
            desc = f"Watchlist Candidate: {row['strategy']} on {row['symbol']} ({row['timeframe']}) shows promise ({row['return_pct']:.2f}%, Z={row['z_score']:.2f})"
            manager.add_insight("watchlist_candidate", desc, 0.6, [str(row['symbol']), str(row['timeframe'])], parameters=params)

    # Detect Negative Anomalies (Failures)
    losers = df[df['z_score'] < -1.5].sort_values('return_pct', ascending=True)
    for _, row in losers.iterrows():
        params = prepare_params(row)
        if row['symbol'] != 'PORTFOLIO':
            desc = f"Performance Warning: {row['strategy']} on {row['symbol']} ({row['timeframe']}) underperformed significantly ({row['return_pct']:.2f}%, Z={row['z_score']:.2f})"
            manager.add_insight("performance_warning", desc, 0.8, [str(row['symbol']), str(row['timeframe'])], parameters=params)

    # Detect Baseline Established (Average Performers: -0.5 <= Z <= 0.5)
    # This ensures new strategies that perform "normally" still get an entry for Forward Test comparison.
    baselines = df[(df['z_score'] >= -0.5) & (df['z_score'] <= 0.5)].sort_values('return_pct', ascending=False)
    for _, row in baselines.iterrows():
        params = prepare_params(row)
        if row['symbol'] != 'PORTFOLIO':
            desc = f"Baseline Established: {row['strategy']} on {row['symbol']} ({row['timeframe']}) returned {row['return_pct']:.2f}% (Z={row['z_score']:.2f})"
            manager.add_insight("baseline_established", desc, 0.5, [str(row['symbol']), str(row['timeframe'])], parameters=params)

    # Portfolio Specific Failures
    portfolio_failures = df[(df['symbol'] == 'PORTFOLIO') & (df['return_pct'] < 0)].sort_values('return_pct', ascending=True)
    for _, row in portfolio_failures.iterrows():
        params = prepare_params(row)
        desc = f"Meta-Strategy Failure: {row['strategy']} Portfolio failed with {row['return_pct']:.2f}% (Z={row['z_score']:.2f})"
        manager.add_insight("meta_strategy_failure", desc, 0.95, ["PORTFOLIO", row['timeframe']], parameters=params)

    # --- 2. Consistency Score ---
    consistency = df.groupby(['strategy', 'symbol', 'timeframe']).agg(
        years_tested=('year', 'count'),
        profitable_years=('return_pct', lambda x: (x > 0).sum()),
        avg_return=('return_pct', 'mean'),
        avg_win_rate=('win_rate', 'mean')
    ).reset_index()
    
    # 1. Consistency Champions (High Win Rate + Consistency)
    # Lowered threshold to 0.65 (65%) to capture robust strategies like HybridRegime (69%)
    # that may have occasional losing years in a 20+ year backtest.
    consistency['consistency_score'] = (consistency['profitable_years'] / consistency['years_tested'])
    consistent_winners = consistency[
        (consistency['consistency_score'] >= 0.65) & 
        (consistency['avg_return'] > 0)
    ]
    
    for _, row in consistent_winners.iterrows():
        if row['years_tested'] >= 3:
            desc = f"Consistent Winner: {row['strategy']} on {row['symbol']} is profitable {(row['consistency_score']*100):.0f}% of years."
            
            # Calculate average win rate for this group
            # We need to go back to the original df to get win_rate for these rows?
            # The 'consistency' df is aggregated.
            # Let's just use the aggregated avg_return we already have.
            # For win_rate, we can re-aggregate or just estimate.
            # Better: Calculate avg_win_rate in the groupby above.
            
            manager.add_insight(
                insight_type="consistency_champion",
                description=desc,
                confidence=0.9,
                scope=[row['symbol']],
                parameters={
                    "strategy": row['strategy'],
                    "symbol": row['symbol'],
                    "avg_annual_return": row['avg_return'],
                    "avg_win_rate": row['avg_win_rate'],
                    "profitable_years": int(row['profitable_years']),
                    "years_tested": int(row['years_tested']),
                    "consistency_score": row['consistency_score']
                }
            )

    if write:
        manager.save_insights()
    else:
        print("\n[Read-Only] Insights not written to file. Use --write to save.")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Analyze Research Results")
    parser.add_argument("--write", action="store_true", help="Write insights to markdown file")
    args = parser.parse_args()
    
    analyze(write=args.write)
