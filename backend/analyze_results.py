import json
import pandas as pd
import numpy as np
import os
from datetime import datetime


MARKDOWN_FILE = '.agent/memory/research_insights.md'

class InsightManager:
    def __init__(self):
        from backend.database import DatabaseManager
        self.db = DatabaseManager()
        self.db.initialize_db() # Ensure table exists
        self.insights = self._load_insights()

    def _load_insights(self):
        return self.db.get_all_insights()

    def save_insights(self):
        for insight in self.insights:
            self.db.save_insight(insight)
        self._generate_markdown()

    def add_insight(self, insight_type, description, confidence, scope, parameters=None, expiration=None):
        # Check for duplicates based on Type and Scope
        # We need to be careful about scope comparison (list vs json string)
        # The loaded insights have scope as list (handled in get_all_insights)
        
        for existing in self.insights:
            # Compare scope sets to be order-independent if needed, or just direct list comparison
            if existing['type'] == insight_type and existing['scope'] == scope:
                # Update existing
                existing['description'] = description
                existing['confidence'] = confidence
                existing['parameters'] = parameters # Update params
                existing['last_updated'] = datetime.now().isoformat()
                print(f"ðŸ”„ Updated Insight: {existing['insight_id']}")
                return

        # Create new ID
        new_id = f"INSIGHT-{len(self.insights) + 1:03d}"
        
        new_insight = {
            "insight_id": new_id,
            "type": insight_type,
            "description": description,
            "confidence": confidence,
            "scope": scope,
            "parameters": parameters,
            "expiration": expiration,
            "status": "active",
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat()
        }
        self.insights.append(new_insight)
        print(f"âœ¨ New Insight Generated: {new_id}")

    def _generate_markdown(self):
        """Auto-generates the human-readable markdown report."""
        content = "# ðŸ§  Research Insights (Auto-Generated)\n\n"
        content += "> [!NOTE]\n"
        content += "> This file is automatically generated by the Analysis Engine. Do not edit manually.\n\n"
        
        # Group by Type
        types = set(i['type'] for i in self.insights)
        
        # Define Sort Order for Types
        type_order = [
            "consistency_champion", 
            "statistical_anomaly", 
            "watchlist_candidate", 
            "meta_strategy_validation", 
            "meta_strategy_failure", 
            "performance_warning",
            "baseline_established"
        ]
        
        sorted_types = sorted(list(types), key=lambda x: type_order.index(x) if x in type_order else 99)
        
        for t in sorted_types:
            content += f"## {t.replace('_', ' ').title()}\n"
            type_insights = [i for i in self.insights if i['type'] == t]
            
            for i in type_insights:
                icon = "ðŸŸ¢" if i['confidence'] >= 0.8 else "ðŸŸ¡" if i['confidence'] >= 0.5 else "ðŸ”´"
                # Handle both 'id' (legacy/new dict) and 'insight_id' (db row)
                # The dict created in add_insight uses 'id', but DB uses 'insight_id'.
                # Let's standardize on 'id' in the in-memory dict for compatibility with this method,
                # or handle both.
                iid = i.get('id', i.get('insight_id'))
                content += f"### {iid} {icon}\n"
                content += f"**{i['description']}**\n\n"
                content += f"- **Confidence**: {i['confidence']}\n"
                content += f"- **Scope**: {', '.join(i['scope'])}\n"
                
                if i.get('parameters'):
                    content += "<details>\n<summary>Strategy Parameters</summary>\n\n"
                    content += "```json\n"
                    content += json.dumps(i['parameters'], indent=2)
                    content += "\n```\n</details>\n"
                
                if i.get('expiration'):
                    content += f"- **Expires**: {i['expiration']}\n"
                if i.get('expiration'):
                    content += f"- **Expires**: {i['expiration']}\n"
                
                # Check for matching Live Session
                # We look for a live session with same Strategy and Symbol
                # (Timeframe is not always in live logs, but usually implied by strategy)
                if hasattr(self, 'live_sessions'):
                    # Filter sessions matching this insight's scope
                    # Scope usually contains [Symbol, Timeframe] or just [Symbol]
                    # Strategy is in parameters
                    
                    insight_strategy = i.get('parameters', {}).get('strategy')
                    insight_symbol = i.get('parameters', {}).get('symbol')
                    
                    # If not in params, try to parse from description or scope
                    if not insight_strategy:
                        # Fallback logic or skip
                        pass
                        
                    matching_sessions = [
                        s for s in self.live_sessions 
                        if s['strategy'] == insight_strategy and s['symbol'] == insight_symbol
                    ]
                    
                    if matching_sessions:
                        # Aggregate or show latest
                        latest_session = matching_sessions[-1] # Show most recent
                        
                        content += "  > **Reality Check (Forward Test)**\n"
                        content += f"  > - **Session**: {latest_session['start_time']} (ID: {latest_session['session_id'][:8]}...)\n"
                        content += f"  > - **Return**: {latest_session['return_pct']:.2f}% (vs Backtest)\n"
                        content += f"  > - **Drawdown**: {latest_session['max_drawdown']:.2f}%\n"
                        content += f"  > - **Trades**: {latest_session['total_trades']}\n"
                        
                        # Status Check
                        # Simple logic: If Return is positive and within 50% of backtest (or better), Verified.
                        # If Return is negative while backtest was positive, Warning.
                        
                        content += "\n"

                content += "\n"
        
        with open(MARKDOWN_FILE, 'w') as f:
            f.write(content)
        print(f"ðŸ“ Report updated: {MARKDOWN_FILE}")

def get_live_sessions():
    """Aggregates live trade logs into session metrics."""
    from backend.database import DatabaseManager
    db = DatabaseManager()
    trades = db.get_live_trades()
    
    if not trades:
        return []
        
    df = pd.DataFrame(trades)
    
    # Group by Session
    sessions = []
    grouped = df.groupby('session_id')
    
    for session_id, group in grouped:
        if group.empty: continue
        
        # Calculate Metrics
        total_pnl = group['pnl'].sum()
        total_trades = len(group)
        wins = len(group[group['pnl'] > 0])
        win_rate = wins / total_trades if total_trades > 0 else 0
        
        # Estimate Return % (Assuming $100k paper account if not tracked)
        # Ideally we'd track initial capital in the session log, but for now we assume standard paper size
        initial_capital = 100000.0 
        return_pct = (total_pnl / initial_capital) * 100
        
        # Max Drawdown (Approximate from trade sequence PnL)
        # Construct a synthetic equity curve
        equity = [initial_capital]
        current_eq = initial_capital
        for pnl in group['pnl']:
            current_eq += pnl
            equity.append(current_eq)
            
        equity_series = pd.Series(equity)
        hwm = equity_series.cummax()
        dd = (hwm - equity_series) / hwm
        max_dd = dd.max() * 100
        
        # Metadata
        first_trade = group.iloc[0]
        last_trade = group.iloc[-1]
        
        sessions.append({
            'session_id': session_id,
            'strategy': first_trade['strategy'],
            'symbol': first_trade['symbol'],
            'start_time': first_trade['timestamp'],
            'end_time': last_trade['timestamp'],
            'return_pct': return_pct,
            'max_drawdown': max_dd,
            'win_rate': win_rate,
            'total_trades': total_trades,
            'pnl': total_pnl
        })
        
    return sessions

def analyze(write=False):
    # Load Results from Database
    from backend.database import DatabaseManager
    db = DatabaseManager()
    try:
        data = db.get_all_test_runs()
        print(f"Loaded {len(data)} test runs from database.")
    except Exception as e:
        print(f"Error loading database: {e}")
        return

    # Convert to DataFrame
    records = []
    for entry in data:
        # Handle both JSON (nested metrics) and SQLite (flat) structures
        if 'metrics' in entry:
            metrics = entry['metrics']
            # Ensure we also get metadata from entry if not in metrics
            strategy = entry.get('strategy', 'Unknown')
            symbol = entry.get('symbol')
            timeframe = entry.get('timeframe')
            year = entry.get('year')
            params = entry.get('parameters', {})
        else:
            # Flat structure (SQLite)
            metrics = entry
            strategy = entry.get('strategy', 'Unknown')
            symbol = entry.get('symbol')
            timeframe = entry.get('timeframe')
            start_date = entry.get('start_date', '')
            year = entry.get('year')
            if not year and start_date:
                year = start_date.split('-')[0]
            params = entry.get('parameters', {})

        records.append({
            'strategy': strategy,
            'symbol': symbol,
            'timeframe': timeframe,
            'year': year,
            'return_pct': metrics.get('return_pct', 0),
            'win_rate': metrics.get('win_rate', 0),
            'drawdown': metrics.get('max_drawdown', 0),
            'trades': metrics.get('total_trades', 0),
            'parameters': params
        })
    
    df = pd.DataFrame(records)
    if df.empty:
        print("No results found.")
        return

    print(f"Loaded {len(df)} test runs.")
    print(f"Loaded {len(df)} test runs.")
    
    # Load Live Sessions
    live_sessions = get_live_sessions()
    print(f"Loaded {len(live_sessions)} live sessions.")
    
    manager = InsightManager()
    
    # Pass live sessions to manager for correlation
    manager.live_sessions = live_sessions

    # --- 1. Outlier Detection (Z-Score) ---
    mean_return = df['return_pct'].mean()
    std_return = df['return_pct'].std()
    df['z_score'] = (df['return_pct'] - mean_return) / std_return
    
    # Detect Positive Anomalies (Winners)
    winners = df[df['z_score'] > 1.5].sort_values('return_pct', ascending=False)
    for _, row in winners.iterrows():
        if row['symbol'] == 'PORTFOLIO':
            desc = f"Meta-Strategy Validation: {row['strategy']} Portfolio returned {row['return_pct']:.2f}% (Z={row['z_score']:.2f})"
            manager.add_insight("meta_strategy_validation", desc, 0.95, ["PORTFOLIO", row['timeframe']], parameters=row['parameters'])
        else:
            desc = f"Statistical Anomaly: {row['strategy']} on {row['symbol']} ({row['timeframe']}) returned {row['return_pct']:.2f}% (Z={row['z_score']:.2f})"
            manager.add_insight("statistical_anomaly", desc, 0.8, [str(row['symbol']), str(row['timeframe'])], parameters=row['parameters'])

    # Detect Watchlist Candidates (Good but not Great: 0.5 < Z < 1.5)
    watchlist = df[(df['z_score'] > 0.5) & (df['z_score'] <= 1.5)].sort_values('return_pct', ascending=False)
    for _, row in watchlist.iterrows():
        if row['symbol'] != 'PORTFOLIO':
            desc = f"Watchlist Candidate: {row['strategy']} on {row['symbol']} ({row['timeframe']}) shows promise ({row['return_pct']:.2f}%, Z={row['z_score']:.2f})"
            manager.add_insight("watchlist_candidate", desc, 0.6, [str(row['symbol']), str(row['timeframe'])], parameters=row['parameters'])

    # Detect Negative Anomalies (Failures)
    losers = df[df['z_score'] < -1.5].sort_values('return_pct', ascending=True)
    for _, row in losers.iterrows():
        if row['symbol'] != 'PORTFOLIO':
            desc = f"Performance Warning: {row['strategy']} on {row['symbol']} ({row['timeframe']}) underperformed significantly ({row['return_pct']:.2f}%, Z={row['z_score']:.2f})"
            manager.add_insight("performance_warning", desc, 0.8, [str(row['symbol']), str(row['timeframe'])], parameters=row['parameters'])

    # Detect Baseline Established (Average Performers: -0.5 <= Z <= 0.5)
    # This ensures new strategies that perform "normally" still get an entry for Forward Test comparison.
    baselines = df[(df['z_score'] >= -0.5) & (df['z_score'] <= 0.5)].sort_values('return_pct', ascending=False)
    for _, row in baselines.iterrows():
        if row['symbol'] != 'PORTFOLIO':
            desc = f"Baseline Established: {row['strategy']} on {row['symbol']} ({row['timeframe']}) returned {row['return_pct']:.2f}% (Z={row['z_score']:.2f})"
            manager.add_insight("baseline_established", desc, 0.5, [str(row['symbol']), str(row['timeframe'])], parameters=row['parameters'])

    # Portfolio Specific Failures
    portfolio_failures = df[(df['symbol'] == 'PORTFOLIO') & (df['return_pct'] < 0)].sort_values('return_pct', ascending=True)
    for _, row in portfolio_failures.iterrows():
        desc = f"Meta-Strategy Failure: {row['strategy']} Portfolio failed with {row['return_pct']:.2f}% (Z={row['z_score']:.2f})"
        manager.add_insight("meta_strategy_failure", desc, 0.95, ["PORTFOLIO", row['timeframe']], parameters=row['parameters'])

    # --- 2. Consistency Score ---
    consistency = df.groupby(['strategy', 'symbol', 'timeframe']).agg(
        years_tested=('year', 'count'),
        profitable_years=('return_pct', lambda x: (x > 0).sum()),
        avg_return=('return_pct', 'mean'),
        avg_win_rate=('win_rate', 'mean')
    ).reset_index()
    
    # 1. Consistency Champions (High Win Rate + Consistency)
    # Lowered threshold to 0.65 (65%) to capture robust strategies like HybridRegime (69%)
    # that may have occasional losing years in a 20+ year backtest.
    consistency['consistency_score'] = (consistency['profitable_years'] / consistency['years_tested'])
    consistent_winners = consistency[
        (consistency['consistency_score'] >= 0.65) & 
        (consistency['avg_return'] > 0)
    ]
    
    for _, row in consistent_winners.iterrows():
        if row['years_tested'] >= 3:
            desc = f"Consistent Winner: {row['strategy']} on {row['symbol']} is profitable {(row['consistency_score']*100):.0f}% of years."
            
            # Calculate average win rate for this group
            # We need to go back to the original df to get win_rate for these rows?
            # The 'consistency' df is aggregated.
            # Let's just use the aggregated avg_return we already have.
            # For win_rate, we can re-aggregate or just estimate.
            # Better: Calculate avg_win_rate in the groupby above.
            
            manager.add_insight(
                insight_type="consistency_champion",
                description=desc,
                confidence=0.9,
                scope=[row['symbol']],
                parameters={
                    "strategy": row['strategy'],
                    "symbol": row['symbol'],
                    "avg_annual_return": row['avg_return'],
                    "avg_win_rate": row['avg_win_rate'],
                    "profitable_years": int(row['profitable_years']),
                    "years_tested": int(row['years_tested']),
                    "consistency_score": row['consistency_score']
                }
            )

    if write:
        manager.save_insights()
    else:
        print("\n[Read-Only] Insights not written to file. Use --write to save.")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="Analyze Research Results")
    parser.add_argument("--write", action="store_true", help="Write insights to markdown file")
    args = parser.parse_args()
    
    analyze(write=args.write)
